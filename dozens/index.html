<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Learning 3D Human Pose From Dozens of Datasets by Bridging Skeleton Formats.">
    <meta name="keywords" content="3D human pose, WACV, WACV23, WACV2023, 2023, Waikoloa, Hawaii, project page, conference, computer vision, machine learning, AI, deep learning, efficientnet, 3D human pose, 3D human understanding, body pose, articulated pose">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning 3D Human Pose From Dozens of Datasets by Bridging Skeleton Formats</title>

    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/prism.css">
    <link rel="stylesheet" href="../css/style.css">
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script defer src="./static/js/prism.js"></script>
</head>
<body>

<header>
    <h1>
        Learning 3D Human Pose Estimation From Dozens of Datasets Using a Geometry-Aware Autoencoder To Bridge Between Skeleton Formats
    </h1>
    <div class="authors">
        <a href="https://istvansarandi.com">István Sárándi</a>,
        <a href="https://scholar.google.de/citations?user=V0iMeYsAAAAJ&hl=en">Alexander Hermans</a>,
        <a href="https://scholar.google.de/citations?user=ZcULDB0AAAAJ&hl=en">Bastian Leibe</a>
    </div>

    <div class="affiliation">RWTH Aachen University</div>
    <div class="venue">WACV 2023</div>

    <div class="links">
        <a href="https://arxiv.org/abs/2212.14474" class="button">
            <span class="icon"><i class="ai ai-arxiv"></i></span>
            arXiv
        </a><a href="sarandi-wacv2023-dozens-poster.pdf" class="button">
            <span class="icon"><i class="fas fa-file-pdf"></i></span>
            Poster
        </a><a href="https://www.youtube.com/watch?v=6IW6oImq3RM" class="button">
            <span class="icon"><i class="fab fa-youtube"></i></span>
            Video
        </a><a href="https://colab.research.google.com/github/isarandi/isarandi.github.io/blob/master/eccv22_demo/demo.ipynb"
           class="button">
            <span class="icon"><i class="fa fa-code"></i></span>
            Google Colab
        </a><a href="#" class="button">
            <span class="icon"><i class="fab fa-github"></i></span>
            GitHub (soon)
        </a>
    </div>
</header>

<h2>TL;DR</h2>
<p class="tldr">
    To enable accurate 3D human pose estimation in downstream research applications,
    we aim to train on every labeled dataset we can get our hands on.
    However, different datasets use incompatible skeleton formats.
    We discover how these skeletons are related using our novel affine-combining autoencoder,
    enabling extreme multi-dataset training on 28 datasets, and thus very strong models.
</p>
<p>Some examples for the different formats:</p>
<div style="text-align:center">
    <video autoplay muted loop playsinline height="100%" width="80%">
        <source src="rotating_hd.mp4" type="video/mp4">
    </video>
</div>

<h2>Talk (4 min.)</h2>
<div class="publication-video">
    <iframe src="https://www.youtube.com/embed/6IW6oImq3RM?rel=0&amp;showinfo=0"
            frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</div>
<p><a href="https://youtu.be/BGkeuef7rIc">Further qualitative videos here.</a></p>

<h2>Method Summary</h2>
<p>We use a 3-step workflow:</p>
<ul>
    <li>
        We first use a multi-task model (which predicts each skeleton on a different output head) to
        generate a pseudo-ground truth "parallel corpus".
    </li>
    <li>
        We use this "parallel corpus" to train an autoencoder that captures the redundancies among
        the skeleton formats by discovering a latent keypoint set.
    </li>
    <li>
        We attach the autoencoder to the end of the pose estimator to regularize its output.
        Alternatively, the latent keypoints can also be predicted directly by the pose estimator.
    </li>
</ul>
<img style="width:100%"  src="steps.png">

<h2>Affine-Combining Autoencoder</h2>
<p>
    One of our contributions is a simple linear autoencoder formulation, where both the encoder and
    the decoder compute affine combinations of its input points.
    This ensures equivariance to rotation and translation (see paper for chirality equivariance).
    Crucially, it also allows knowledge transfer of skeleton relations from the 2D to the depth axis
    (2D is more accurate in the pseudo-GT).
    Furthermore, the information bottleneck of this autoencoder is a list of geometrically
    interpretable latent 3D keypoints that can also be directly predicted and can serve as an
    interface between different skeleton formats.
</p>

<div style="text-align: center">
    <img src="acae_formulas.png" style="width:70%">
</div>

<h2>Results</h2>
<p>
    In the paper we first show that scaling up the data indeed helps.
    Then we show that the skeleton consistency improves with our proposed consistency
    regularization.
</p>
<p>
    But what's even more important from the application point of view is that <strong>the final
    models are MUCH stronger than currently available models</strong> that are trained on less data:
</p>
<img style="width:100%" src="sota_table.png"/>

<h2>You Can Try It!</h2>
<a href="https://colab.research.google.com/github/isarandi/isarandi.github.io/blob/master/eccv22_demo/demo.ipynb"
   target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
<pre><code class="language-python"># 'pip install tensorflow tensorflow-hub' if not yet installed
import tensorflow as tf
import tensorflow_hub as hub

model = hub.load('https://bit.ly/metrabs_l') # or _s
image = tf.image.decode_image(tf.io.read_file('test.jpg')) # image can have any size
preds = model.detect_poses(image, skeleton='smpl+head_30')
preds['boxes'] # shape: [num_people, 5], the 5 are [x_left, y_top, width, height, confidence]
preds['poses3d'] # shape: [num_people, 30, 3], in millimeters
preds['poses2d'] # shape: [num_people, 30, 2], in pixels</code></pre>
<p>
    More options in the <a href="https://github.com/isarandi/metrabs/blob/master/docs/API.md">API
    docs</a>.
</p>

<p>
    The following models are available according to backbone and crop resolution, all using YOLOv4 as a built-in detector:
</p>
<ul>
    <li>EfficientNetV2-S, 256 px: <code>hub.load('https://bit.ly/metrabs_s_256')</code></li>
    <li>EfficientNetV2-S, 384 px: <code>hub.load('https://bit.ly/metrabs_s')</code></li>
    <li>EfficientNetV2-L, 384 px: <code>hub.load('https://bit.ly/metrabs_l')</code></li>
    <li>EfficientNetV2-XL, 384 px: <code>hub.load('https://bit.ly/metrabs_xl')</code></li>
</ul>

<p>
    <strong>Real-time capability:</strong> The EffNetV2-S model runs at 28 FPS without batching on the RTX 3090 consumer GPU, when processing the multiperson MuPoTS-3D dataset.
</p>

<p>
    Code for dataset preprocessing and training the ACAE is coming soon!
</p>

<h2>Paper Abstract</h2>
<p>
    Deep learning-based 3D human pose estimation performs best when trained on large amounts of
    labeled data, making combined learning from many datasets an important research direction. One
    obstacle to this endeavor are the different skeleton formats provided by different datasets,
    i.e., they do not label the same set of anatomical landmarks. There is little prior research on
    how to best supervise one model with such discrepant labels. We show that simply using separate
    output heads for different skeletons results in inconsistent depth estimates and insufficient
    information sharing across skeletons. As a remedy, we propose a novel affine-combining
    autoencoder (ACAE) method to perform dimensionality reduction on the number of landmarks. The
    discovered latent 3D points capture the redundancy among skeletons, enabling enhanced
    information sharing when used for consistency regularization. Our approach scales to an extreme
    multi-dataset regime, where we use 28 3D human pose datasets to supervise one model, which
    outperforms prior work on a range of benchmarks, including the challenging 3D Poses in the Wild
    (3DPW) dataset. Our code and models are available for research purposes.
</p>
<p><a href="https://arxiv.org/abs/2212.14474">Full paper here.</a></p>

<h2>Citation</h2>
<pre class="bibtex">@inproceedings{Sarandi2023dozens,
    author = {S\'ar\'andi, Istv\'an and Hermans, Alexander and Leibe, Bastian},
    title = {Learning {3D} Human Pose Estimation from Dozens of Datasets using a Geometry-Aware Autoencoder to Bridge Between Skeleton Formats},
    booktitle = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    year = {2023}
}</pre>

<p>As the base pose estimator model, we use our earlier MeTRAbs in this work:</p>
<pre class="bibtex">@article{Sarandi2021metrabs,
    title = {{MeTRAbs:} Metric-Scale Truncation-Robust Heatmaps for Absolute {3D} Human Pose Estimation},
    author = {S\'ar\'andi, Istv\'an and Linder, Timm and Arras, Kai O. and Leibe, Bastian},
    journal = {IEEE Transactions on Biometrics, Behavior, and Identity Science (T-BIOM)},
    volume = {3},
    number = {1},
    pages = {16--30},
    year = {2021}
}</pre>

<footer>
    <p>
        The pose estimator models can only be used for non-commercial research.
    </p>
    <p>
        <strong>Acknowledgments.</strong> This work was supported by the ERC Consolidator Grant project “DeeViSe”
        (ERC-CoG-2017-773161) and by Robert Bosch GmbH under the project “Context Understanding for
        Autonomous Systems.”
    </p>
</footer>

</body>
</html>
